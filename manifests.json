[{"id": "vicuna-7b-q4", "name": "Vicuna 7B Q4", "description": "Vicuna is an open-source chatbot, fine-tuned on LLaMA using conversations from ShareGPT. Developed collaboratively by a team from UC Berkeley, CMU, Stanford, and UC San Diego, this auto-regressive language model aims to advance research in large language models and chatbots. Primarily designed for researchers and hobbyists in AI and NLP, the model undergoes preliminary evaluations using diverse questions and GPT-4 for output judgment. The current model has been 4-bit quantized using ggml framework. [Learn More](https://vicuna.lmsys.org/)", "documentation": "# Documentation\n\n## Description\n\nVicuna is an open-source chatbot, fine-tuned on LLaMA using conversations from ShareGPT. Developed collaboratively by a team from UC Berkeley, CMU, Stanford, and UC San Diego, this auto-regressive language model aims to advance research in large language models and chatbots. Primarily designed for researchers and hobbyists in AI and NLP, the model undergoes preliminary evaluations using diverse questions and GPT-4 for output judgment. The current model has been 4-bit quantized using ggml framework. [Learn More](https://vicuna.lmsys.org/)\n\n## Example Usage\n\n> Can you explain what is a large language model?\n\n> Can you give some examples applications?\n\n> Are there any limitations?\n\n> Summarize the above in two sentences.\n\n> Write me a story about a superstar.\n\n## Technical Details\n### Training / Fine Tuning Costs\n\nAfter a significant effort spanning four days and a substantial expenditure of $800 for GPU rentals from providers such as Lambda Labs and Paperspace, the resulting model, named gpt4all-lora, is now up and running. This sum includes the costs incurred from several unsuccessful training attempts. In addition to the GPU costs, a further $500 was expended on the OpenAI API. However, with the model's release, the training duration has been drastically reduced. Now, the entire training process can be completed in approximately eight hours using Lambda Labs' DGX A100 8x 80GB setup, at a significantly reduced total cost of $100.\n\n### Default Parameters\n\nFor our experiments, we have been using the following parameters:\n\n```python\ntemperature=0.2\ntop_p=0.95\nstop=[]\nmax_tokens=256\nrepeat_penalty=1.1\n```\n### Inference Benchmarks\n\n### Quality Benchmarks\n\nFor more information concerning Vicuna evaluation method you can refer to https://github.com/lm-sys/FastChat/tree/main/fastchat/eval.\n\n### Serving Details\n\nIn order to expose the service we are currently using FastAPI and llama-cpp-python library https://abetlen.github.io/llama-cpp-python/ which is compatible with all ggml models.\n\n```txt\nllama-cpp-python==0.1.43\n```\n\n### Embeddings\n\nThe current model supports Embeddings generation too. Another endpoint is exposed for this purpose. You can check out the documentation for each container to see how to use it at http://{container_ip}:8000/docs or at our public services Open API doc at https://mock.prem.ninja/docs\n\n## License\n\nThe model is a research preview intended for non-commercial use only, subject to the model License of LLaMA, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT. ", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/chat-vicuna-7b-q4/logo.svg", "modelInfo": {"weightsName": "vicuna-7b-q4.bin", "weightsSize": 4212859520, "memoryRequirements": 8192, "streaming": true}, "interfaces": ["chat", "embeddings"], "dockerImages": {"cpu": {"size": 8986449745, "image": "ghcr.io/premai-io/chat-vicuna-7b-q4-cpu:0.0.2"}}, "defaultPort": 8000}, {"id": "gpt4all-lora-q4", "name": "GPT4ALL-Lora Q4", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/chat-gpt4all-lora-q4/logo.svg", "description": "GPT4All, developed by Nomic AI, is a chatbot trained over an extensive corpus of assistant interactions. By finetuning LLaMA 7B, GPT4All provides an open-source ecosystem to train and deploy efficient, assistant-style large language models locally on consumer-grade CPUs. This democratized approach to AI aims to bolster open research, reproducibility, and promote developments in AI alignment and interpretability. The current model has been 4-bit quantized using ggml framework. [Learn More](https://github.com/nomic-ai/gpt4all).", "documentation": "# Documentation\n\n## Description\n\nGPT4All, developed by Nomic AI, is a chatbot trained over an extensive corpus of assistant interactions. By finetuning LLaMA 7B, GPT4All provides an open-source ecosystem to train and deploy efficient, assistant-style large language models locally on consumer-grade CPUs. This democratized approach to AI aims to bolster open research, reproducibility, and promote developments in AI alignment and interpretability. The current model has been 4-bit quantized using ggml framework. [Learn More](https://github.com/nomic-ai/gpt4all).\n\n## Example Usage\n\n> Can you explain what is a large language model?\n\n> Can you give some examples applications?\n\n> Are there any limitations?\n\n> Summarize the above in two sentences.\n\n> Write me a story about a superstar.\n\n## Technical Details\n\n### Training / Fine Tuning Costs\n\nAfter a significant effort spanning four days and a substantial expenditure of $800 for GPU rentals from providers such as Lambda Labs and Paperspace, the resulting model, named gpt4all-lora, is now up and running. This sum includes the costs incurred from several unsuccessful training attempts. In addition to the GPU costs, a further $500 was expended on the OpenAI API. However, with the model's release, the training duration has been drastically reduced. Now, the entire training process can be completed in approximately eight hours using Lambda Labs' DGX A100 8x 80GB setup, at a significantly reduced total cost of $100.\n\n### Default Parameters\n\nFor our experiments, we have been using the following parameters:\n\n```python\ntemperature=0.2\ntop_p=0.95\nstop=[]\nmax_tokens=256\nrepeat_penalty=1.1\n```\n\n### Inference Benchmarks\n\n### Quality Benchmarks\n\nFor more information about GPT4All performances and quality you can visit: https://gpt4all.io/index.html.\n\n### Serving Details\n\nIn order to expose the service we are currently using FastAPI and llama-cpp-python library https://abetlen.github.io/llama-cpp-python/ which is compatible with all ggml models.\n\n```txt\nllama-cpp-python==0.1.43\n```\n\n### Embeddings\n\nThe current model supports Embeddings generation too. Another endpoint is exposed for this purpose. You can check out the documentation for each container to see how to use it at http://{container_ip}:8000/docs or at our public services Open API doc at https://mock.prem.ninja/docs\n\n## License\n\nThe model is a research preview intended for non-commercial use only, subject to the model License of LLaMA, Terms of Use of the data generated by OpenAI.", "modelInfo": {"weightsName": "vicuna-7b-q4.bin", "weightsSize": 4212859520, "memoryRequirements": 8192, "streaming": true}, "interfaces": ["chat", "embeddings"], "dockerImages": {"cpu": {"size": 8986459988, "image": "ghcr.io/premai-io/chat-gpt4all-lora-q4-cpu:0.0.2"}}, "defaultPort": 8000}, {"id": "qdrant", "name": "Qdrant", "description": "Qdrant is a vector similarity search engine designed for storing, searching, and managing points along with their respective payloads. Built with an emphasis on extensive filtering, it is particularly beneficial for neural network matching, semantic-based matching, and faceted search. Qdrant offers various deployment options including local mode, on-premise server deployment, and Qdrant Cloud, each catering to different use-case scenarios. [Learn More](https://qdrant.tech/documentation/)", "documentation": "# Documentation\n\n## Description\n\nQdrant is a vector similarity search engine designed for storing, searching, and managing points along with their respective payloads. Built with an emphasis on extensive filtering, it is particularly beneficial for neural network matching, semantic-based matching, and faceted search. Qdrant offers various deployment options including local mode, on-premise server deployment, and Qdrant Cloud, each catering to different use-case scenarios. [Learn More](https://qdrant.tech/documentation/)\n\n## Example Usage\n\nThe service can be used with Langchain or the official qdrant python client (https://github.com/qdrant/qdrant). Below you can find an example using the service with Langchain. In the code snippet, we are assuming that you are using all-miniLM-l6-v2 model for embeddings generation and the service is running locally on port 8001.\n\n```python\n\n!pip install qdrant-client\n\nimport os\nimport getpass\nimport openai\n\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\n\nembeddings = OpenAIEmbeddings()\n\nurl = \"http://localhost:6333\"\nqdrant = Qdrant.from_documents(\n    docs, embeddings, \n    url, prefer_grpc=True, \n    collection_name=\"my_documents\",\n)\n\nquery = \"What did the president say about Ketanji Brown Jackson\"\nfound_docs = qdrant.similarity_search(query)\n\nprint(found_docs[0].page_content)\n```\n", "interfaces": ["store"], "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/store-qdrant/logo.svg", "modelInfo": {}, "volumePath": "/qdrant/storage", "dockerImages": {"cpu": {"size": 126913893, "image": "qdrant/qdrant:v1.0.3"}}, "defaultPort": 6333}, {"id": "redis-vector-db", "name": "Redis Vector DB", "description": "Redis, short for Remote Dictionary Server, serves as a multifunctional in-memory data structure store. It functions as a distributed key-value database, cache, and message broker, all operating in-memory for high-speed data access. With optional durability, Redis ensures data persistence despite potential system failures. [Learn More](https://redis.com/solutions/use-cases/vector-database/)", "documentation": "# Documentation\n\n## Description\n\nRedis, short for Remote Dictionary Server, serves as a multifunctional in-memory data structure store. It functions as a distributed key-value database, cache, and message broker, all operating in-memory for high-speed data access. With optional durability, Redis ensures data persistence despite potential system failures. Learn more https://redis.com/solutions/use-cases/vector-database/.\n\n## Example Usage\n\nThe service can be used with Langchain. You can check the official documentation at this link: https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/redis.html. In the code snippet, we are assuming that you are using all-miniLM-l6-v2 model for embeddings generation and the service is running locally on port 8001.\n\n```python\n\n!pip install redis\n\nimport os\nimport getpass\n\nos.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.redis import Redis\n\nfrom langchain.document_loaders import TextLoader\n\nloader = TextLoader('../../../state_of_the_union.txt')\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\n\nembeddings = OpenAIEmbeddings()\n\nrds = Redis.from_documents(docs, embeddings, redis_url=\"redis://localhost:6379\",  index_name='link')\n```", "interfaces": ["store"], "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/store-redis/logo.svg", "modelInfo": {}, "volumePath": "/data", "dockerImages": {"cpu": {"size": 261514099, "image": "redis/redis-stack-server:latest"}}, "defaultPort": 6379}, {"id": "all-minilm-l6-v2", "name": "All-MiniLM-L6-v2", "description": "All-MiniLM-L6-v2 is a sentence-transformers model designed to map sentences and paragraphs to a 384-dimensional dense vector space, ideal for clustering or semantic search tasks. Developed during Hugging Face's Community week, this model is fine-tuned on a 1B sentence pairs dataset with a contrastive learning objective. It excels in encoding short texts, capturing semantic information, and is useful for information retrieval, clustering, or sentence similarity tasks. [Learn More](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)", "documentation": "# Documentation\n\n## Description\n\nAll-MiniLM-L6-v2 is a sentence-transformers model designed to map sentences and paragraphs to a 384-dimensional dense vector space, ideal for clustering or semantic search tasks. Developed during Hugging Face's Community week, this model is fine-tuned on a 1B sentence pairs dataset with a contrastive learning objective. It excels in encoding short texts, capturing semantic information, and is useful for information retrieval, clustering, or sentence similarity tasks.\n\n## Example Usage\n\nThe service is compatible with Langchain and more specifically it follows OpenAI API request / response format. Below you can find an example for using the service with Langchain.\n\n```python\nEXAMPLE_HERE=None\n```\n\nYou can also check the official sentence transformers documentation at https://www.sbert.net/. It provides extensive examples and detailed information for using the model.\n\n## License\n\nThe model is under Apache License.\n", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/embeddings-all-minilm-l6-v2/logo.svg", "modelInfo": {"memoryRequirements": 8192}, "devices": ["cpu", "gpu"], "dockerImages": {"cpu": {"size": 1238261448, "image": "ghcr.io/premai-io/embeddings-all-minilm-l6-v2-cpu:0.0.2"}, "gpu": {"size": 20687072077, "image": "ghcr.io/premai-io/embeddings-all-minilm-l6-v2-gpu:0.0.1"}}, "interfaces": ["embeddings"], "defaultPort": 8000}, {"id": "dolly-v2-12", "name": "Dolly-v2-12b", "description": "Dolly-v2-12b, developed by Databricks, is an instruction-following large language model trained on diverse capability domains. It exhibits remarkable instruction following behavior, surpassing the foundation model it's based on, Pythia-12b. The model is particularly designed for commercial use and its inference can be run on various GPU configurations. [Learn More](https://huggingface.co/databricks/dolly-v2-12b)", "documentation": "# Documentation\n\n## Description\n\nDolly-v2-12b, developed by Databricks, is an instruction-following large language model trained on diverse capability domains. It exhibits remarkable instruction following behavior, surpassing the foundation model it's based on, Pythia-12b. The model is particularly designed for commercial use and its inference can be run on various GPU configurations. [Learn More](https://huggingface.co/databricks/dolly-v2-12b)\n\n## Hardware Requirements\n\n## Example Usage\n\n## Fine Tuning Instructions & Cost\n\n## Inference Benchmarks\n", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/chat-dolly-v2-12b/logo.svg", "modelInfo": {"memoryRequirements": 24576, "streaming": false}, "interfaces": ["chat"], "dockerImages": {"gpu": {"size": 40689160223, "image": "ghcr.io/premai-io/chat-dolly-v2-12b-gpu:0.0.1"}}, "defaultPort": 8000}]