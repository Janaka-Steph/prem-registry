[{"id": "stable-diffusion-2-1", "name": "Stable Diffusion 2.1", "description": "", "documentation": "# Documentation", "comingSoon": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/diffuser-stable-diffusion-2-1/logo.svg", "modelInfo": {}, "interfaces": ["diffuser"], "dockerImages": {}, "defaultPort": 8000, "defaultExternalPort": 9111}, {"id": "whisper-tiny", "name": "Whisper Tiny", "description": "", "documentation": "# Documentation", "comingSoon": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/audio-to-text-whisper-tiny/logo.svg", "modelInfo": {}, "interfaces": ["audio-to-text"], "dockerImages": {}, "defaultPort": 8000, "defaultExternalPort": 10111}, {"id": "redis-vector-db", "name": "Redis Vector DB", "description": "Redis, short for Remote Dictionary Server, serves as a multifunctional in-memory data structure store. It functions as a distributed key-value database, cache, and message broker, all operating in-memory for high-speed data access. With optional durability, Redis ensures data persistence despite potential system failures. [Learn More](https://redis.com/solutions/use-cases/vector-database/)", "documentation": "# Documentation\n\n## Description\n\nRedis, short for Remote Dictionary Server, serves as a multifunctional in-memory data structure store. It functions as a distributed key-value database, cache, and message broker, all operating in-memory for high-speed data access. With optional durability, Redis ensures data persistence despite potential system failures. Learn more https://redis.com/solutions/use-cases/vector-database/.\n\n## Getting Started\n\nThe service can be used with Langchain. You can check the official documentation at this link: https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/redis.html. In the code snippet, we are assuming that you are using all-miniLM-l6-v2 model for embeddings generation and the service is running locally on port 8001.\n\n```python\n\n!pip install redis\n\nimport os\n\nfrom langchain.chains import LLMChain\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.docstore.document import Document\nfrom langchain.vectorstores.redis import Redis\nfrom langchain.prompts import PromptTemplate\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\ndoc1 = Document(page_content=\"Prem is an easy to use open source AI platform. With Prem you can quickly build provacy preserving AI applications.\")\ndoc2 = Document(page_content=\"\"\"\nPrem App\n\nAn intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.\n\n\"\"\")\ndoc3 = Document(page_content=\"\"\"\nPrem Benefits\n\nEffortless Integration\nSeamlessly implement machine learning models with the user-friendly interface of OpenAI's API.\n\nReady for the Real World\nBypass the complexities of inference optimizations. Prem's got you covered.\n\nRapid Iterations, Instant Results\nDevelop, test, and deploy your models in just minutes.\n\nPrivacy Above All\nYour keys, your models. We ensure end-to-end encryption.\n\nComprehensive Documentation\nDive into our rich resources and learn how to make the most of Prem.\n\nPreserve Your Anonymity\nMake payments with Bitcoin and Cryptocurrency. It's a permissionless infrastructure, designed for you.\n\"\"\")\n\n# Using sentence transformers all-MiniLM-L6-v2\nembeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8001/api/v1\")\n\n# Using locally running Redis\nurl = \"redis://localhost:6379\"\n\nrds = Redis.from_documents(docs, embeddings, redis_url=url,  index_name=\"prem_index_test\")\n\nquery = \"What are Prem Benefits?\"\ndocs = vectorstore.similarity_search(query)\nprint(docs[0].page_content)\n```", "interfaces": ["vector-store"], "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/vector-store-redis/logo.svg", "modelInfo": {}, "volumePath": "/data", "dockerImages": {"cpu": {"size": 261514099, "image": "redis/redis-stack-server:latest"}}, "defaultPort": 6379, "defaultExternalPort": 6379}, {"id": "vicuna-7b-q4", "name": "Vicuna 7B Q4", "description": "Vicuna is an open-source chatbot, fine-tuned on LLaMA using conversations from ShareGPT. Developed collaboratively by a team from UC Berkeley, CMU, Stanford, and UC San Diego, this auto-regressive language model aims to advance research in large language models and chatbots. Primarily designed for researchers and hobbyists in AI and NLP, the model undergoes preliminary evaluations using diverse questions and GPT-4 for output judgment. The current model has been 4-bit quantized using ggml framework.", "documentation": "# Documentation\n\n## Description\n\nVicuna is an open-source chatbot, fine-tuned on LLaMA using conversations from ShareGPT. Developed collaboratively by a team from UC Berkeley, CMU, Stanford, and UC San Diego, this auto-regressive language model aims to advance research in large language models and chatbots. Primarily designed for researchers and hobbyists in AI and NLP, the model undergoes preliminary evaluations using diverse questions and GPT-4 for output judgment. The current model has been 4-bit quantized using ggml framework <a href='https://vicuna.lmsys.org/' target='_blank'>Learn More</a>.\n\n## Example Usage\n\nCan you explain what is a large language model?\n\n```markdown\nSure! A large language model is a class of machine learning models that are trained on very large amounts of text data. The goal of training these models is to create a representation of human language that can be used for various natural language processing tasks, such as text generation, question- Answering and sentiment analysis. These models typically consist of several layers of neural networks, with each layer representing a different aspect of language (e.g., word meaning, syntax, context). As the model gets larger, it has the ability to learn more abstract representations of language that are more representative of human language understanding.\n```\n\nCan you give some examples applications?\n\n```markdown\nLarge language models have a wide range of applications in natural language processing, here are a few examples:\n\n1. Text generation: Large language models can be used to generate text that sounds like it was written by a human, such as chatbot responses or summaries of long articles.\n2. Question-Answering: Large language models can be trained to answer questions based on the information in a large dataset of text. This is often used in virtual assistants and search engines.\n3. Sentiment analysis: Large language models can be used to understand the sentiment behind a piece of text, such as a customer review or social media post.\n4. Translation: Large language models can be trained to translate text from one language to another, using large amounts of training data from both languages.\n5. Dialogue System: Large language model can help in building dialogue system which can have human-like conversations with users.\n6. Language Modeling: Large language models are used for language modeling task where they are able to mimic the way humans produce and process natural language.\n```\n\nAre there any limitations?\n\n```markdown\nWhile large language models have many benefits, there are also some limitations that should be considered:\n\n1. Large datasets required: Training a large language model requires access to vast amounts of text data, which can be difficult or expensive to obtain.\n2. Computational resources required: Training and using large language models requires significant computational resources, which may not always be available.\n3. Privacy concerns: Training large language models on personal data can raise privacy concerns. It is important to ensure that the data used for training is handled responsibly and ethically.\n```\n\nSummarize the above in two sentences.\n\n```markdown\nLarge language models have many benefits, but also require access to vast amounts of text data, computational resources, and should be trained with responsible and ethical handling of personal data.\n```\n\nWrite me a story about a superstar.\n\n```markdown\nOnce upon a time, there was a young man named Jack who dreamed of becoming a superstar. Despite facing numerous obstacles and setbacks, he never gave up on his goal.\n\nOne day, an opportunity came knocking at his door. A record label scout approached him with a contract, recognizing his talent and potential. Jack was overjoyed and eagerly signed the deal.\n\nAs his career began to take off, Jack found himself traveling all around the world on tour. He met many other talented musicians and artists along the way, but none as kind and generous as his\n```\n\n## Technical Details\n\n### Default Parameters\n\nFor our experiments, we have been using the following parameters:\n\n```python\ntemperature=0.2\ntop_p=0.95\nstop=[]\nmax_tokens=256\nrepeat_penalty=1.1\n```\n\n### Inference Benchmarks\n\n### Quality Benchmarks\n\nFor more information concerning Vicuna evaluation method you can refer to <a href='https://github.com/lm-sys/FastChat/tree/main/fastchat/eval' target='_blank'></a>.\n\n### Serving Details\n\nIn order to expose the service we are currently using FastAPI and llama-cpp-python library <a href='https://abetlen.github.io/llama-cpp-python/' target='_blank'></a> which is compatible with all ggml models.\n\n```python\nllama-cpp-python==0.1.43\n```\n\n### Embeddings\n\nThe current model supports Embeddings generation too. Another endpoint is exposed for this purpose. You can check out the documentation for each container to see how to use it at `http://IP:PORT/docs` or at our public services Open API doc at <a href='https://mock.prem.ninja/docs' target='_blank'></a>\n\n### Getting Started with Langchain\n\n```python\n!pip install langchain\n!pip install openai\n\nimport os\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8111/api/v1\", max_tokens=128)\n\nmessages = [\n    HumanMessage(content=\"Can you explain what is a large language model?\")\n]\nchat(messages)\n```\n\n## License\n\nThe model is a research preview intended for non-commercial use only, subject to the model License of LLaMA, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT. ", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/chat-vicuna-7b-q4/logo.svg", "modelInfo": {"weightsName": "vicuna-7b-q4.bin", "weightsSize": 4212859520, "maxCtxTokens": 512, "memoryRequirements": 8192, "streaming": true}, "interfaces": ["chat", "embeddings"], "dockerImages": {"cpu": {"size": 8986449745, "image": "ghcr.io/premai-io/chat-vicuna-7b-q4-cpu:0.0.5"}}, "defaultPort": 8000, "defaultExternalPort": 8111}, {"id": "stable-diffusion-2", "name": "Stable Diffusion 2", "description": "", "documentation": "# Documentation", "comingSoon": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/diffuser-stable-diffusion-2/logo.svg", "modelInfo": {}, "interfaces": ["diffuser"], "dockerImages": {}, "defaultPort": 8000, "defaultExternalPort": 9222}, {"id": "gpt4all-lora-q4", "name": "GPT4ALL Lora Q4", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/chat-gpt4all-lora-q4/logo.svg", "description": "GPT4All, developed by Nomic AI, is a chatbot trained over an extensive corpus of assistant interactions. By finetuning LLaMA 7B, GPT4All provides an open-source ecosystem to train and deploy efficient, assistant-style large language models locally on consumer-grade CPUs. This democratized approach to AI aims to bolster open research, reproducibility, and promote developments in AI alignment and interpretability. The current model has been 4-bit quantized using ggml framework.", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\nGPT4All, developed by Nomic AI, is a chatbot trained over an extensive corpus of assistant interactions. By finetuning LLaMA 7B, GPT4All provides an open-source ecosystem to train and deploy efficient, assistant-style large language models locally on consumer-grade CPUs. This democratized approach to AI aims to bolster open research, reproducibility, and promote developments in AI alignment and interpretability. The current model has been 4-bit quantized using ggml framework. [Learn more](https://github.com/nomic-ai/gpt4all).\n\n## \ud83d\udcd2 Example Usage\n\n### 1\ufe0f\u20e3 Prompt: Why do I need to run machine learning models on-premise?\n\n> There are several reasons why you might want to run machine learning models on-premise. One reason is that it can provide greater control over the data and model, which can be important for certain use cases. Another reason is that it can allow for faster execution times, as there may not be a need to wait for cloud infrastructure or resources. Additionally, some organizations may have regulatory requirements that require them to keep sensitive data on-premise rather than in the cloud.\n\n### 2\ufe0f\u20e3 Prompt: How do I run my models on-premise?\n\n> You can run your models on-premise by installing the necessary software and hardware requirements. You will need to have a server or a cluster of servers with sufficient processing power, memory, storage space, and network bandwidth to handle the computational load of your model. Additionally, you may also require specialized hardware such as GPUs for accelerated computing. Once you have all the required resources in place, you can then install the necessary software packages and configure them according to your needs.\n\n### 3\ufe0f\u20e3 Prompt: What are the limitations of running my models on-premise?\n\n> There are several limitations when running models on-premise such as high upfront costs for hardware and software, limited scalability due to physical constraints, and a lack of flexibility in terms of deployment options. Additionally, maintaining and upgrading the infrastructure can be time-consuming and costly.\n\n### 4\ufe0f\u20e3 Prompt: What are the trade-offs of deploying models on-premise I should be aware of?\n\n> There are several trade-offs to consider when deploying models on-premise such as increased costs, limited scalability, and a lack of flexibility in terms of deployment options. Additionally, maintaining and upgrading the infrastructure can be time-consuming and costly.\n\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\udd27 Training / Fine Tuning Costs\n\n> After a significant effort spanning four days and a substantial expenditure of $800 for GPU rentals from providers such as Lambda Labs and Paperspace, the resulting model, named gpt4all-lora, is now up and running. This sum includes the costs incurred from several unsuccessful training attempts. In addition to the GPU costs, a further $500 was expended on the OpenAI API. \n\nHowever, with the model's release, the training duration has been drastically reduced. Now, the entire training process can be completed in approximately eight hours using Lambda Labs' DGX A100 8x 80GB setup, at a significantly reduced total cost of $100.\n\n### \ud83d\udd22 Default Parameters\n\nFor our experiments, we have been using the following parameters:\n\n```python\ntemperature=0.2\ntop_p=0.95\nstop=[]\nmax_tokens=256\nrepeat_penalty=1.1\n```\n\n### Inference Benchmarks\n\n### \ud83d\udd0e Quality Benchmarks\n\nFor more information about GPT4All performances and quality you can visit: https://gpt4all.io/index.html.\n\n### \ud83d\ude80 Serving Details\n\nIn order to expose the service we are currently using FastAPI and [llama-cpp-python](https://abetlen.github.io/llama-cpp-python/) library which is compatible with all ggml models.\n\n```python\nllama-cpp-python==0.1.43\n```\n\n### \u26aa\ufe0f Embeddings\n\nThe current model supports Embeddings generation too. Another endpoint is exposed for this purpose. You can check out the documentation for each container to see how to use it at `http://{container_ip}:8000/docs` or at our public services [Open API documentation](https://mock.prem.ninja/docs).\n\n### \ud83e\udd9c\ud83d\udd17 Getting Started with Langchain\n\n```python\n!pip install langchain\n!pip install openai\n\nimport os\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8222/api/v1\", max_tokens=128)\n\nmessages = \n    HumanMessage(content=\"Why do I need to run machine learning models on-premise?\")\n]\n\nchat(messages)\n```\n\n## \ud83d\udcdc License\n\nThe model is a research preview intended for **non-commercial use only**, subject to the model License of LLaMA, Terms of Use of the data generated by OpenAI.", "modelInfo": {"weightsName": "vicuna-7b-q4.bin", "weightsSize": 4212859520, "maxCtxTokens": 512, "memoryRequirements": 8192, "streaming": true}, "interfaces": ["chat", "embeddings"], "dockerImages": {"cpu": {"size": 8986459988, "image": "ghcr.io/premai-io/chat-gpt4all-lora-q4-cpu:0.0.5"}}, "defaultPort": 8000, "defaultExternalPort": 8222}, {"id": "all-minilm-l6-v2", "name": "All MiniLM L6 v2", "description": "All-MiniLM-L6-v2 is a sentence-transformers model designed to map sentences and paragraphs to a 384-dimensional dense vector space, ideal for clustering or semantic search tasks. Developed during Hugging Face's Community week, this model is fine-tuned on a 1B sentence pairs dataset with a contrastive learning objective. It excels in encoding short texts, capturing semantic information, and is useful for information retrieval, clustering, or sentence similarity tasks. [Learn More](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)", "documentation": "# Documentation\n\n## Description\n\nAll-MiniLM-L6-v2 is a sentence-transformers model designed to map sentences and paragraphs to a 384-dimensional dense vector space, ideal for clustering or semantic search tasks. Developed during Hugging Face's Community week, this model is fine-tuned on a 1B sentence pairs dataset with a contrastive learning objective. It excels in encoding short texts, capturing semantic information, and is useful for information retrieval, clustering, or sentence similarity tasks.\n\n## Getting Started\n\nThe service is compatible with Langchain and more specifically it follows OpenAI API request / response format. Below you can find an example for using the service with Langchain.\n\n```python\nimport os\n\nfrom langchain.embeddings import OpenAIEmbeddings\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nembeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8444/api/v1\")\ntext = \"Prem is an easy to use open source AI platform.\"\nquery_result = embeddings.embed_query(text)\ndoc_result = embeddings.embed_documents([text])\n```\n\nYou can also check the official sentence transformers documentation at https://www.sbert.net/. It provides extensive examples and detailed information for using the model.\n\n## License\n\nThe model is under Apache License.\n", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/embeddings-all-minilm-l6-v2/logo.svg", "modelInfo": {"memoryRequirements": 8192}, "dockerImages": {"cpu": {"size": 1238261448, "image": "ghcr.io/premai-io/embeddings-all-minilm-l6-v2-cpu:0.0.5"}, "gpu": {"size": 20687072077, "image": "ghcr.io/premai-io/embeddings-all-minilm-l6-v2-gpu:0.0.1"}}, "interfaces": ["embeddings"], "defaultPort": 8000, "defaultExternalPort": 8444}, {"id": "whisper-large-v2", "name": "Whisper Large v2", "description": "", "documentation": "# Documentation", "comingSoon": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/audio-to-text-whisper-large-v2/logo.svg", "modelInfo": {}, "interfaces": ["audio-to-text"], "dockerImages": {}, "defaultPort": 8000, "defaultExternalPort": 10111}, {"id": "qdrant", "name": "Qdrant", "description": "Qdrant is a vector similarity search engine designed for storing, searching, and managing points along with their respective payloads. Built with an emphasis on extensive filtering, it is particularly beneficial for neural network matching, semantic-based matching, and faceted search. Qdrant offers various deployment options including local mode, on-premise server deployment, and Qdrant Cloud, each catering to different use-case scenarios. [Learn More](https://qdrant.tech/documentation/)", "documentation": "# Documentation\n\n## Description\n\nQdrant is a vector similarity search engine designed for storing, searching, and managing points along with their respective payloads. Built with an emphasis on extensive filtering, it is particularly beneficial for neural network matching, semantic-based matching, and faceted search. Qdrant offers various deployment options including local mode, on-premise server deployment, and Qdrant Cloud, each catering to different use-case scenarios. [Learn More](https://qdrant.tech/documentation/)\n\n## Getting Started\n\nThe service can be used with Langchain or the official qdrant python client (https://github.com/qdrant/qdrant). Below you can find an example using the service with Langchain. In the code snippet, we are assuming that you are using all-miniLM-l6-v2 model for embeddings generation and the service is running locally on port 8001.\n\n```python\n\n!pip install qdrant-client\n\nimport os\n\nfrom langchain.chains import LLMChain\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.docstore.document import Document\nfrom langchain.vectorstores import Qdrant\nfrom langchain.prompts import PromptTemplate\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\ndoc1 = Document(page_content=\"Prem is an easy to use open source AI platform. With Prem you can quickly build provacy preserving AI applications.\")\ndoc2 = Document(page_content=\"\"\"\nPrem App\n\nAn intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.\n\n\"\"\")\ndoc3 = Document(page_content=\"\"\"\nPrem Benefits\n\nEffortless Integration\nSeamlessly implement machine learning models with the user-friendly interface of OpenAI's API.\n\nReady for the Real World\nBypass the complexities of inference optimizations. Prem's got you covered.\n\nRapid Iterations, Instant Results\nDevelop, test, and deploy your models in just minutes.\n\nPrivacy Above All\nYour keys, your models. We ensure end-to-end encryption.\n\nComprehensive Documentation\nDive into our rich resources and learn how to make the most of Prem.\n\nPreserve Your Anonymity\nMake payments with Bitcoin and Cryptocurrency. It's a permissionless infrastructure, designed for you.\n\"\"\")\n\n# Using sentence transformers all-MiniLM-L6-v2\nembeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8001/api/v1\")\n\n# Using locally running Qdrant\nurl = \"http://localhost:6333\"\n\nvectorstore = Qdrant.from_documents(\n    [doc1, doc2, doc3], \n    embeddings, \n    url=url, \n    collection_name=\"prem_collection_test\",\n)\n\nquery = \"What are Prem Benefits?\"\ndocs = vectorstore.similarity_search(query)\nprint(docs[0].page_content)\n```\n", "interfaces": ["vector-store"], "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/vector-store-qdrant/logo.svg", "modelInfo": {}, "volumePath": "/qdrant/storage", "dockerImages": {"cpu": {"size": 126913893, "image": "qdrant/qdrant:v1.0.3"}}, "defaultPort": 6333, "defaultExternalPort": 6333}, {"id": "bark", "name": "Bark", "description": "", "documentation": "# Documentation", "comingSoon": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/text-to-audio-bark/logo.svg", "modelInfo": {}, "interfaces": ["text-to-audio"], "dockerImages": {}, "defaultPort": 8000, "defaultExternalPort": 10111}, {"id": "replit-v1-3b", "name": "Replit v1 3B", "description": "", "documentation": "# Documentation", "comingSoon": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/copilot-replit-v1-3b/logo.svg", "modelInfo": {}, "interfaces": ["copilot"], "dockerImages": {}, "defaultPort": 8000, "defaultExternalPort": 10111}, {"id": "dolly-v2-12", "name": "Dolly-v2-12b", "beta": true, "description": "Dolly-v2-12b, developed by Databricks, is an instruction-following large language model trained on diverse capability domains. It exhibits remarkable instruction following behavior, surpassing the foundation model it's based on, Pythia-12b. The model is particularly designed for commercial use and its inference can be run on various GPU configurations.", "documentation": "# Documentation\n\n## Description\n\nDolly-v2-12b, developed by Databricks, is an instruction-following large language model trained on diverse capability domains. It exhibits remarkable instruction following behavior, surpassing the foundation model it's based on, Pythia-12b. The model is particularly designed for commercial use and its inference can be run on various GPU configurations. <a href='https://huggingface.co/databricks/dolly-v2-12b' target='_blank'>Learn More</a>\n\n## Hardware Requirements\n\n## Example Usage\n\n## Fine Tuning Instructions & Cost\n\n## Inference Benchmarks\n", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/chat-dolly-v2-12b/logo.svg", "modelInfo": {"memoryRequirements": 24576, "streaming": false}, "interfaces": ["chat"], "dockerImages": {"gpu": {"size": 40689160223, "image": "ghcr.io/premai-io/chat-dolly-v2-12b-gpu:0.0.1"}}, "defaultPort": 8000, "defaultExternalPort": 8333}, {"id": "codet5p-220m-py", "name": "Saleforce 5P 220M Python", "description": "", "documentation": "# Documentation", "comingSoon": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/copilot-codet5p-220m-py/logo.svg", "modelInfo": {}, "interfaces": ["copilot"], "dockerImages": {}, "defaultPort": 8000, "defaultExternalPort": 10111}]