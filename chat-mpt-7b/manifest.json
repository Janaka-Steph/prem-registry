{
    "id": "mpt-7b",
    "name": "MPT 7B",
    "beta": true,
    "description": "MPT 7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. The model is particularly designed for commercial use (License: Apache-2.0) and its inference can be run on various GPU configurations.",
    "documentation": "",
    "icon": "",
    "modelInfo": {
        "memoryRequirements": 13455,
        "tokensPerSecond": 40
    },
    "interfaces": [
        "chat"
    ],
    "dockerImages": {
        "gpu": {
            "size": 33040731243,
            "image": "ghcr.io/premai-io/mpt-7b-gpu:1.0.0"
        }
    },
    "defaultPort": 8000,
    "defaultExternalPort": 8451
}